{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "We would be applying logistic regression to a classification involving more than one class using the ones vs all method which involves training a seperate classfier for every class with respect to the full training set.\n",
    "In this exercise, we would be applying this in handwritten digit recognition. The dataset consist of pixel intensity matrix for a training set of 5000 examples each consisting of a seperate hand written digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from scipy import optimize\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 400)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sio.loadmat('ex3data1.mat')\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 400 features here, i.e a single training example contains 20 by 20 pixels such that each feature contains the grey scale intensity of that point in floating points. Using logistic regression with a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD8CAYAAABJsn7AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18b3604d198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(5,5)\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        ax[i,j].imshow(X[np.random.randint(1, 5000)].reshape([20,20]).T, cmap = 'gray')\n",
    "\n",
    "plt.setp([a.get_xticklabels() for a in ax[0, :]], visible=False)\n",
    "plt.setp([a.get_yticklabels() for a in ax[:, 1]], visible=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 401)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the constant element\n",
    "\n",
    "X = np.hstack(((np.ones([X.shape[0],1])), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the labels into a Matrix of 0 and 1 based on the class\n",
    "\n",
    "Y = np.zeros((y.shape[0], 10))\n",
    "\n",
    "for i,j in enumerate(y):\n",
    "    Y[i,(j-1)] = 1\n",
    "\n",
    "# This result in a class label Matrix Y which would be used for our ones vs all classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    input: a value or an array of values.\n",
    "    Returns: The output of the sigmoid function operated on the input value.\n",
    "    '''\n",
    "    \n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def cost_reg(theta, X, y, lambd):\n",
    "    '''\n",
    "    Inputs: \n",
    "    theta - The parameter vector for the decision boundary.\n",
    "    X - The feature matrix for the features.\n",
    "    y - The labels corresponding to the features.\n",
    "    lambd - Regularization coefficient\n",
    "    \n",
    "    Returns: The cost\n",
    "    '''\n",
    "    theta = theta.reshape((theta.shape[0],1))\n",
    "    m = len(y)\n",
    "    h = sigmoid(np.matmul(X, theta))\n",
    "    cost = (1/m) * np.sum(-y * np.log(h) - (1 - y) * np.log(1 - h))\n",
    "    cost += (lambd/(2*m)) * np.power(theta,2) #this takes into account the regularization term, thus increasing the cost or penalty for the selected parameters\n",
    "    return cost\n",
    "\n",
    "def grad_reg(theta, X, y, lambd):\n",
    "    '''\n",
    "    theta - The parameter vector for the decision boundary.\n",
    "    X - The feature matrix for the features.\n",
    "    y - The labels corresponding to the features.\n",
    "    lambd - Regularization coefficient\n",
    "    \n",
    "    Returns: The gradient or differential values corresponding to the theta parameters.\n",
    "    '''\n",
    "    theta = theta.reshape((theta.shape[0],1))\n",
    "    m = len(y)\n",
    "    h = sigmoid(np.matmul(X, theta))\n",
    "    grad = (1/m) * sum(((h - y) * X))\n",
    "    theta[0,0] = 0\n",
    "    theta = theta.flatten()\n",
    "    grad += (lambd/m) * theta # To account for regularization. The constant term is usually not regularized.\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def predict (X, y, theta):\n",
    "    t = sigmoid(np.matmul(X,theta))\n",
    "    ypred = np.array([1 if i >= 0.5 else 0 for i in t]) #gives the predicted values\n",
    "    y = y.flatten()\n",
    "    acc = (ypred == y)\n",
    "    acc = acc.astype(int)\n",
    "    predictAccuracy = np.mean(acc)\n",
    "    return predictAccuracy * 100\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets implement logistic regression for class labels of one only and see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(X.shape[1])\n",
    "lambdaa = 0.1 #the regularization parameter\n",
    "thetaopt = optimize.minimize(fun = cost_reg, x0 = theta, jac = grad_reg, \\\n",
    "                             args = (X, Y[:,0][np.newaxis].T, lambdaa), method = 'L-BFGS-B').x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predict accuracy for the model for class label 1 is: 98.56 based on the training dataset\n"
     ]
    }
   ],
   "source": [
    "print('The predict accuracy for the model for class label 1 is: %.2f based on the training dataset' \\\n",
    "      % predict(X, Y[:,0][np.newaxis].T, thetaopt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already stated, the concept of ones vs all trains different classifies for each class label. In each case, it considers this class label as the positive class while the others are negative. Just as we have done above, let's use a for loop to train a theta parameter n by c matrix; where c is the number of class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimation complete for class label 1. Predict accuracy on training set is: 98.560 \n",
      "Optimation complete for class label 2. Predict accuracy on training set is: 97.180 \n",
      "Optimation complete for class label 3. Predict accuracy on training set is: 96.460 \n",
      "Optimation complete for class label 4. Predict accuracy on training set is: 98.000 \n",
      "Optimation complete for class label 5. Predict accuracy on training set is: 98.040 \n",
      "Optimation complete for class label 6. Predict accuracy on training set is: 98.500 \n",
      "Optimation complete for class label 7. Predict accuracy on training set is: 98.420 \n",
      "Optimation complete for class label 8. Predict accuracy on training set is: 92.360 \n",
      "Optimation complete for class label 9. Predict accuracy on training set is: 95.100 \n",
      "Optimation complete for class label 10. Predict accuracy on training set is: 98.820 \n"
     ]
    }
   ],
   "source": [
    "theta = np.zeros([X.shape[1],10]) #initialization of the Theta Matrix\n",
    "lambdaa = 0.1\n",
    "predictlst = []\n",
    "\n",
    "for i, thetai in enumerate(theta.T):\n",
    "    theta[:,i] = optimize.minimize(fun = cost_reg, x0 = thetai, jac = grad_reg, args = (X, Y[:,i][np.newaxis].T, lambdaa), method = 'L-BFGS-B').x\n",
    "    accuracy = predict(X, Y[:,i][np.newaxis].T, theta[:,i])\n",
    "    print(\"Optimation complete for class label %d. Predict accuracy on training set is: %.3f \" % ((i+1), accuracy))\n",
    "    predictlst.append(accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training performance of our model can obtained using the weighted average accuracy of each class model. We would use a different approach to determine the overall accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted accuracy of our model on the training set is: 90.960\n"
     ]
    }
   ],
   "source": [
    "t = sigmoid(np.matmul(X, theta))\n",
    "ypred = np.argmax(t, axis = 1) + 1 #Adding one would account for zero based index\n",
    "acc = (ypred ==  y.flatten()).astype(int)\n",
    "predictAccuracy = np.mean(acc) * 100\n",
    "\n",
    "print('The predicted accuracy of our model on the training set is: %.3f' % (predictAccuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now attempt to solve the same problem above using a neural network.\n",
    "The first step would choosing an good architecture for our neural network. This network would obviously have 400 units in its input layer (sinces there are 400 features in the trainings set) and 10 units in its output layer (since there are different classes).\n",
    "The number of hidden layers and number of hidden units in each hidden layer is based on our descretion on the complexity of the model which is required to properly classify our training examples.\n",
    "\n",
    "We would choose 1 hidden layers with 25 units.\n",
    "Our neural network is similar to logistic regression, but with a more complex model. Due to the added complexity of extra layers, we would need to re-implement the cost function and the gradient function. However, the gradient estimated via a process called back-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The forward propagation step\n",
    "def nncost(nnparams, inputsize, hiddensize, num_labels, X, Y, lambdaa):\n",
    "    '''\n",
    "    Inputs:\n",
    "    -------\n",
    "    nnparams: for the sake of simplicity, the parameter vectors are unrolled or flattened into a single vector. Not that the \n",
    "        each layer of the network has its set of parameters.\n",
    "    inputsize: number of input features, this determines the size of the input layer.\n",
    "    hiddensize: The size of the hidden layer.\n",
    "    num_labels: This determines the size of the output layer or the final layer of the neural network.\n",
    "    lambdaa: regularization parameter\n",
    "    X: feature matrix\n",
    "    Y: Classes or label matrix\n",
    "    \n",
    "    Output: Returns the cost\n",
    "    -------\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Reshape the flattened out parameters\n",
    "    theta1 = nnparams[:hiddensize * (inputsize + 1)].reshape([hiddensize, (inputsize + 1)])\n",
    "    theta2 = nnparams[(hiddensize * (inputsize + 1)):].reshape([num_labels, (hiddensize + 1)])\n",
    "    \n",
    "    #y_matrix = np.eye(len(y))[y-1,:] #create a matrix of the labels, similar to what was done with a for loop\n",
    "    \n",
    "    # First Layer of Network\n",
    "    a1 = np.hstack(((np.ones([X.shape[0],1])), X))\n",
    "    z1 = np.matmul(a1, theta1.T)\n",
    "    a1out = sigmoid(z1)\n",
    "    \n",
    "    # Second Layer of Network (Hidden Layer)\n",
    "    a2 = np.hstack(((np.ones([a1out.shape[0],1])), a1out))\n",
    "    z2 = np.matmul(a2, theta2.T)\n",
    "    a2out = sigmoid(z2)\n",
    "    \n",
    "    # Compute cost un-regularized using the output from the NN\n",
    "    h1 = (-Y * np.log(a2out)) - ((1 - Y) * np.log(1 - a2out))\n",
    "    h_unreg = (1/m) * (np.sum(h1, axis = (0,1)))\n",
    "    \n",
    "    # Regularize the cost function\n",
    "    # Set the regularization parameters corresponding to the bias units\n",
    "    theta1[:,0] = 0 \n",
    "    theta2[:,0] = 0\n",
    "    h = h_unreg + (lambdaa/(2*m))*((np.sum(np.power(theta1,2), axis = (0,1))) + (np.sum(np.power(theta2,2), axis = (0,1))))\n",
    "    \n",
    "    return h\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidgradient(z):\n",
    "    '''\n",
    "    Input: A matrix or a scalar value\n",
    "    Returns: The derivative is of the sigmoid function\n",
    "    -------\n",
    "    '''\n",
    "    \n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def nngrad(nnparams, inputsize, hiddensize, num_labels, X, Y, lambdaa):\n",
    "    '''\n",
    "    Inputs:\n",
    "    -------\n",
    "    nnparams: for the sake of simplicity, the parameter vectors are unrolled or flattened into a single vector. Not that the \n",
    "        each layer of the network has its set of parameters.\n",
    "    inputsize: number of input features, this determines the size of the input layer.\n",
    "    hiddensize: The size of the hidden layer.\n",
    "    num_labels: This determines the size of the output layer or the final layer of the neural network.\n",
    "    lambdaa: regularization parameter\n",
    "    X: feature matrix\n",
    "    Y: Classes or label matrix\n",
    "    \n",
    "    Output: Returns the gradient via back propagation\n",
    "    -------\n",
    "    '''\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Reshape the flattened out parameters\n",
    "    theta1 = nnparams[:hiddensize * (inputsize + 1)].reshape([hiddensize, (inputsize + 1)])\n",
    "    theta2 = nnparams[(hiddensize * (inputsize + 1)):].reshape([num_labels, (hiddensize + 1)])\n",
    "    \n",
    "    #y_matrix = np.eye(len(y))[y-1,:] #create a matrix of the labels, similar to what was done with a for loop\n",
    "    \n",
    "    # First Layer of Network\n",
    "    a1 = np.hstack(((np.ones([X.shape[0],1])), X))\n",
    "    z1 = np.matmul(a1, theta1.T)\n",
    "    a1out = sigmoid(z1)\n",
    "    \n",
    "    # Second Layer of Network (Hidden Layer)\n",
    "    a2 = np.hstack(((np.ones([a1out.shape[0],1])), a1out))\n",
    "    z2 = np.matmul(a2, theta2.T)\n",
    "    a2out = sigmoid(z2)\n",
    "    \n",
    "    # Backpropagation for output layer\n",
    "    d3 = a2out - Y\n",
    "    Delta2 = np.matmul(d3.T, a2)\n",
    "    \n",
    "    # Backpropagation for hidden layer\n",
    "    d2 = np.matmul(d3, theta2[:,1:]) * sigmoidgradient(z1)\n",
    "    Delta1 = np.matmul(d2.T, a1)\n",
    "    \n",
    "    # Set the regularization parameters corresponding to the bias units\n",
    "    theta1[:,0] = 0 \n",
    "    theta2[:,0] = 0\n",
    "    \n",
    "    theta1_grad = (1/m) * Delta1 + (lambdaa/m) * theta1\n",
    "    theta2_grad = (1/m) * Delta2 + (lambdaa/m) * theta2\n",
    "    \n",
    "    return np.concatenate([theta1_grad.flatten(), theta2_grad.flatten()])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step would be initialization of the learning parameters (theta). Before now we are used to initializing theta with zeros, however in Neural networks, this doesn't work out well. Its a normal practice to break symmetry by using random parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initparams(sizeout, sizein):\n",
    "    '''\n",
    "    Input: \n",
    "    -----\n",
    "    sizeout: The number of output neurons to the layer of the NN\n",
    "    sizein: The number of input neurons to the layer of the NN\n",
    "    \n",
    "    Output:\n",
    "    ------\n",
    "    Returns the unrolled paramaters for the layer of the NN\n",
    "    \n",
    "    '''\n",
    "    epsilon = sqrt(6)/sqrt(sizeout + sizein)\n",
    "    return np.random.rand(sizeout, 1 + sizein) * 2 * epsilon - epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets put it all together and train our first neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "numofinputunits = 400\n",
    "numofhiddenunits = 25\n",
    "numoflabels = 10\n",
    "lambdaa = 0.1\n",
    "\n",
    "theta1 = initparams(numofhiddenunits, numofinputunits) # The first layer of NN\n",
    "theta2 = initparams(numoflabels, numofhiddenunits) # The second layer of NN\n",
    "\n",
    "nnparams = np.concatenate([theta1.flatten(), theta2.flatten()]) # Unroll the parameters into a vector to be fed to the minimization algorithm\n",
    "\n",
    "nnparamsopt = optimize.minimize(fun = nncost, x0 = nnparams, jac = nngrad, \\\n",
    "                             args = (numofinputunits, numofhiddenunits, numoflabels, X, Y, lambdaa), method = 'L-BFGS-B').x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnpredict(nnparams, numofinputunits, numofhiddenunits, num_labels, X):\n",
    "    '''\n",
    "    Inputs:\n",
    "    -------\n",
    "    nnparams - trained NN parameters\n",
    "    numofinputunits: number of input features, this determines the size of the input layer.\n",
    "    numofhiddenunits: The size of the hidden layer.\n",
    "    num_labels: This determines the size of the output layer or the final layer of the neural network.\n",
    "    X - The features vector\n",
    "    \n",
    "    Output:\n",
    "    -------\n",
    "    Returns the predicted values of y\n",
    "    '''\n",
    "    \n",
    "    # Unroll the parameter vectors\n",
    "    theta1 = nnparams[:numofhiddenunits * (numofinputunits + 1)].reshape([numofhiddenunits, (numofinputunits + 1)])\n",
    "    theta2 = nnparams[(numofhiddenunits * (numofinputunits + 1)):].reshape([num_labels, (numofhiddenunits + 1)])\n",
    "    \n",
    "    # Predict First layer\n",
    "    h1 = sigmoid(np.matmul(np.hstack(((np.ones([X.shape[0],1])), X)), theta1.T))\n",
    "    \n",
    "    # Predict Second Layer\n",
    "    h2 = sigmoid(np.matmul(np.hstack(((np.ones([h1.shape[0],1])), h1)), theta2.T))\n",
    "    \n",
    "    return np.argmax(h2, axis = 1) + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted accuracy of our model on the training set is: 99.940\n"
     ]
    }
   ],
   "source": [
    "ypred = nnpredict(nnparamsopt, numofinputunits, numofhiddenunits, numoflabels, X)\n",
    "acc = (ypred ==  y.flatten()).astype(int)\n",
    "predictAccuracy = np.mean(acc) * 100\n",
    "\n",
    "print('The predicted accuracy of our model on the training set is: %.3f' % (predictAccuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model accuracy is almost perfect due to the added complexity of an extra layer of the neural network compared with a linear model logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
